{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you're going to implement GloVe, one of the more popular, effective, and efficient approaches to learning word embeddings. You should use the [paper](http://nlp.stanford.edu/pubs/glove.pdf) for reference, and you're welcome to look to other implementations for guidance as long as the code that you submit is your own and it fits the basic structure provided by the starter code below.\n",
    "\n",
    "Submit your completed notebook through NYU Classes by 9:30 AM on October 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to run GloVe on the text of the Stanford Sentiment Treebank (SST) training set. Usually these methods are run on extremely large corpora, but we're using this here to make sure that you can train a reasonable model without waiting for hours or days. \n",
    "\n",
    "First, let's load the data as before. For our purposes, we won't need either the labels or any of the test or dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sst_home = '../trees'\n",
    "\n",
    "import re\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(sst_home + '/train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's count cooccurrences on the training set. We'll use a nine-word window. Along the way, we'll also collect a list of all of the index pairs $(i,j)$ that have a count greater than zero. \n",
    "\n",
    "There are faster ways to do this, but this works for a small corpus like ours. To speed up GloVe training below, though, we'll only consider the 250 most frequent words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def tokenize(string):\n",
    "    return string.split()\n",
    "\n",
    "word_counter = collections.Counter()\n",
    "for example in training_set:\n",
    "    word_counter.update(tokenize(example['text']))\n",
    "vocabulary = [pair[0] for pair in word_counter.most_common()[0:250]]\n",
    "index_to_word_map = dict(enumerate(vocabulary))\n",
    "word_to_index_map = dict([(index_to_word_map[index], index) for index in index_to_word_map])\n",
    "\n",
    "def extract_cooccurrences(dataset, word_map, amount_of_context=4):\n",
    "    num_words = len(vocabulary)\n",
    "    cooccurrences = np.zeros((num_words, num_words))\n",
    "    nonzero_pairs = set()\n",
    "    for example in dataset:\n",
    "        words = tokenize(example['text'])\n",
    "        for target_index in range(len(words)):\n",
    "            target_word = words[target_index]\n",
    "            if target_word not in word_to_index_map:\n",
    "                continue\n",
    "            target_word_index = word_to_index_map[target_word]\n",
    "            min_context_index = max(0, target_index - amount_of_context)\n",
    "            max_word = min(len(words), target_index + amount_of_context)\n",
    "            for context_index in range(min_context_index, target_index) + range(target_index + 1, max_word):\n",
    "                context_word = words[context_index]\n",
    "                if context_word not in word_to_index_map:\n",
    "                    continue\n",
    "                context_word_index = word_to_index_map[context_word]\n",
    "                cooccurrences[target_word_index][context_word_index] += 1.0\n",
    "                nonzero_pairs.add((target_word_index, context_word_index))\n",
    "    return cooccurrences, list(nonzero_pairs)\n",
    "                \n",
    "cooccurrences, nonzero_pairs = extract_cooccurrences(training_set, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementation (60%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up evalation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be frank, a GloVe model trained on such a small dataset and vocabulary won't be spectacular, so we won't bother with a full-fledged similarity or analogy evaluation. Instead, we'll use the simple scoring function below, which grades the model on how well it captures ten easy/simple similarity comparisons. The function returns a score between 0 and 10. Random embeddings can be expected to get a score of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def similarity(word_one, word_two):\n",
    "    vec_one = model.get_embedding(word_to_index_map[word_one]).reshape(1, -1)\n",
    "    vec_two = model.get_embedding(word_to_index_map[word_two]).reshape(1, -1)\n",
    "    return float(cosine_similarity(vec_one, vec_two))\n",
    "\n",
    "def score(model):\n",
    "    score = 0\n",
    "    score += similarity('a', 'an') > similarity('a', 'documentary')\n",
    "    score += similarity('in', 'of') > similarity('in', 'picture')\n",
    "    score += similarity('action', 'thriller') >  similarity('action', 'end')\n",
    "    score += similarity('films', 'movies') > similarity('films', 'almost')\n",
    "    score += similarity('film', 'movie') > similarity('film', 'movies')\n",
    "    score += similarity('script', 'plot') > similarity('script', 'big')\n",
    "    score += similarity('watch', 'see') > similarity('watch', 'down')\n",
    "    score += similarity('``', \"''\") > similarity('``', 'quite')\n",
    "    score += similarity('funny', 'entertaining') > similarity('funny', 'seems')\n",
    "    score += similarity('good', 'great') > similarity('good', 'minutes')\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've built and trained the model, you can evaluate it as follows. You may not have to, though, since evaluation is built into the model code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some starter code below for training a TensorFlow model. **Fill it out to create an implementation of GloVe, then train it on the SST training set.**\n",
    "\n",
    "Try not to modify any of the starter code. Leaving it as is will make sure that you'll be able to use my pre-chosen hyperparameter values, which should save you lots of time, since this model is slow to train and hyperparameter tuning is therefore fairly tedious.\n",
    "\n",
    "Some tips:\n",
    "\n",
    "- Make sure to fill in all of the TODO lines below.\n",
    "- You should use minibatch SGD (the starter code is set up for it), and you should run computation for an entire minibatch at a time using a single `sess.run()` call. This means that many of your variables will have an extra batch dimension in addition to the dimensions that you'd expect from reading the paper. \n",
    "- Every time you define any new TF computation, add a comment indicating what shape (/dimensions) you expect the result to be. Use `tf.Print()` and `tf.shape()` to make sure that you're getting what you expect.\n",
    "- You'll likely need to use `tf.reshape()` and `tf.batch_matmul()` at least once each.\n",
    "- If you're new to TF, try to find a partner or group to work with. The solution is fairly simple, but finding it may not be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "class glove:\n",
    "    def __init__(self, num_words):\n",
    "        # Define the hyperparameters\n",
    "        self.dim = 10                # The size of the learned embeddings\n",
    "        self.alpha = 0.75            # One of the hyperparameters defining the scaling function F\n",
    "        self.xmax = 50               # One of the hyperparameters defining the scaling function F \n",
    "        self.learning_rate = 1.0     # SGD LR - Much higher than you'll see for typical NNs, but it works here\n",
    "        self.batch_size = 1024       # Somewhat arbitrary - can be tuned, but often tuned for speed, not accuracy\n",
    "        self.training_epochs = 2000  # This model should train faster per epoch than the logistic regression models,\n",
    "                                     # so we can afford to run for more epochs. You should feel free to stop the model\n",
    "                                     # during training if it seems to stop improving.\n",
    "        self.display_epoch_freq = 25 # How often to test and print out statistics\n",
    "        self.num_words = num_words   # The number of vectors to learn\n",
    "        self.embeddings = None       # To be set later\n",
    "    \n",
    "        # Define the inputs to the model\n",
    "        pass # TODO: Replace\n",
    "    \n",
    "        # Define the trainable parameters of the model\n",
    "        pass # TODO: Replace\n",
    "    \n",
    "        # Define the forward computation of the model\n",
    "        # The final result that you compute should be a 1024-dimensional vector of example-by-example \n",
    "        # cost function values called `self.example_cost`.\n",
    "        pass # TODO: Replace\n",
    "        \n",
    "        # Define the cost function (here, the exp and sum are built in)\n",
    "        self.total_cost = tf.reduce_mean(self.example_cost)\n",
    "        \n",
    "        # This library call performs the main SGD update equation\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.total_cost)\n",
    "        \n",
    "        # Create an operation to fill zero values in for W and b\n",
    "        self.init = tf.initialize_all_variables()\n",
    "        \n",
    "        # Initialize the model\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(self.init)\n",
    "        \n",
    "    def train(self, cooccurrences, nonzero_pairs, num_words):\n",
    "        self.embeddings = None  # If we restart training, make sure to clear the cached embeddings\n",
    "        print 'Training.'\n",
    "        \n",
    "        # Training cycle - In one epoch, we'll visit each nonzero entry in cooccurrences once\n",
    "        for epoch in range(self.training_epochs):\n",
    "            random.shuffle(nonzero_pairs)\n",
    "\n",
    "            avg_cost = 0.\n",
    "            total_batches = int(len(nonzero_pairs) / self.batch_size)\n",
    "            \n",
    "            # Loop over all batches in epoch\n",
    "            for i in range(total_batches):\n",
    "                # Assemble a minibatch dictionary to feed to `sess.run()`\n",
    "                feed = {}\n",
    "                pass # TODO: Replace\n",
    "\n",
    "                # Run the optimizer to take a gradient step, and also fetch the value of the \n",
    "                # cost function for logging\n",
    "                _, c = self.sess.run([self.optimizer, self.total_cost], \n",
    "                                     feed_dict=feed)\n",
    "                                                                    \n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batches\n",
    "                \n",
    "            # Display some statistics about the step\n",
    "            if (epoch+1) % self.display_epoch_freq == 0:\n",
    "                self.cache_embeddings()  # Make sure we run scoring with a fresh copy of the embeddings\n",
    "                print \"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \"Score:\", score(self)\n",
    "\n",
    "    def cache_embeddings(self):\n",
    "        # Fill in self.embeddings with a matrix (in NumPy format) containing one vector per word, \n",
    "        # representing the final output of GloVe. It should be possible to index into that\n",
    "        # matrix using `get_embedding` below.\n",
    "        self.embeddings = None  # TODO: Replace\n",
    "\n",
    "    def get_embedding(self, index):\n",
    "        if self.embeddings is None:\n",
    "            cache_embeddings()\n",
    "        return self.embeddings[index, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a working model (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should use the following commands to train the model for at least 2000 steps. If your model works, it will usually converge to a score of 10 within that many steps.\n",
    "\n",
    "Tips:\n",
    "\n",
    "- You cannot run this until you've completed the previous code block. That's because the starter code doesn't actually define at trainable model.\n",
    "- The score and cost for the first few hundred epochs of training will vary quite a bit due to the random initialization. The real measure of success is how the model does once it nears convergence.\n",
    "- Make sure to show the full output for a real run in your notebook when you submit. I will not retrain your model to grade it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = glove(len(vocabulary))  # Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.train(cooccurrences, nonzero_pairs, len(vocabulary))  # Train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Questions (40%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in your answers below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1:** What do the entries on the diagonal of the `cooccurrences` matrix represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** Deleting the weighting function $F$ should hurt the performance of the model. Why would you expect this to be the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3:** What would you expect to happen if you used a learning rate of 0.0001. Why? (You're welcome to try it, but that shouldn't be the sole basis for your answer.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4:** If you used 100 times more training data, would the model take 100 times as long to train? Just as long as now? Somewhere in between? Choose one and (informally) defend your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
