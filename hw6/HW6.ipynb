{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW6: A GRU-pair model for SNLI with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we'll build and train an attention-based model for SNLI. The model will incorporate the basic word-by-word attention+matching layer design from [Wang and Jiang](https://arxiv.org/pdf/1512.08849v2.pdf), but it will use GRUs instead of LSTMs, and it will use [Luong et al.](http://www.aclweb.org/anthology/D15-1166)'s 'general' bilinear attention scoring function.\n",
    "\n",
    "Do not start this assignment at the last minute, since the model you build may take several hours to train.\n",
    "\n",
    "**Note:** Submissions (both late initial submissions and resubmissions) will only be accepted until the evening of 12/16. Resubmission will be open for only a few days, even for students who submit on time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to download and unzip SNLI, which you can find [here](http://nlp.stanford.edu/projects/snli/). Set `snli_home` below to point to it. The following block of code loads it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "snli_home = '../snli_1.0'\n",
    "\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "\n",
    "LABEL_MAP = {\n",
    "    \"entailment\": 0,\n",
    "    \"neutral\": 1,\n",
    "    \"contradiction\": 2\n",
    "}\n",
    "\n",
    "def load_snli_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            loaded_example = json.loads(line)\n",
    "            if loaded_example[\"gold_label\"] not in LABEL_MAP:\n",
    "                continue\n",
    "            loaded_example[\"label\"] = LABEL_MAP[loaded_example[\"gold_label\"]]\n",
    "            data.append(loaded_example)\n",
    "        random.seed(1)\n",
    "        random.shuffle(data)\n",
    "    return data\n",
    "     \n",
    "training_set = load_snli_data(snli_home + '/snli_1.0_train.jsonl')\n",
    "dev_set = load_snli_data(snli_home + '/snli_1.0_dev.jsonl')\n",
    "test_set = load_snli_data(snli_home + '/snli_1.0_test.jsonl')\n",
    "\n",
    "# Note: Unlike with k-nearest neighbors, evaluation here should be fast, and we don't need to\n",
    "# trim down the dev and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll convert the data to index vectors in the same way that we've done for in-class exercises with RNN-based sentiment models. A few notes:\n",
    "\n",
    "- We use a sequence length of only 10, which is short enough that we're truncating a large fraction of sentences.\n",
    "- Tokenization is easy here because we're relying on the output of a parser (which does tokenization as part of parsing), just as with the SST corpus that we've been using until now. Note that we use the 'sentence1_binary_parse' field of each example rather than the human-readable 'sentence1'.\n",
    "- We're using a moderately large vocabulary of about 36k words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 10\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def sentences_to_padded_index_sequences(datasets):\n",
    "    '''Annotates datasets with feature vectors.'''\n",
    "    \n",
    "    PADDING = \"<PAD>\"\n",
    "    UNKNOWN = \"<UNK>\"\n",
    "    \n",
    "    # Extract vocabulary\n",
    "    def tokenize(string):\n",
    "        string = re.sub(r'\\(|\\)', '', string)\n",
    "        return string.lower().split()\n",
    "    \n",
    "    word_counter = collections.Counter()\n",
    "    for example in datasets[0]:\n",
    "        word_counter.update(tokenize(example['sentence1_binary_parse']))\n",
    "        word_counter.update(tokenize(example['sentence2_binary_parse']))\n",
    "        \n",
    "    vocabulary = set([word for word in word_counter])\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary = [PADDING, UNKNOWN] + vocabulary\n",
    "        \n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    indices_to_words = {v: k for k, v in word_indices.items()}\n",
    "        \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            for sentence in ['sentence1_binary_parse', 'sentence2_binary_parse']:\n",
    "                example[sentence + '_index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n",
    "\n",
    "                token_sequence = tokenize(example[sentence])\n",
    "                padding = SEQ_LEN - len(token_sequence)\n",
    "\n",
    "                for i in range(SEQ_LEN):\n",
    "                    if i >= padding:\n",
    "                        if token_sequence[i - padding] in word_indices:\n",
    "                            index = word_indices[token_sequence[i - padding]]\n",
    "                        else:\n",
    "                            index = word_indices[UNKNOWN]\n",
    "                    else:\n",
    "                        index = word_indices[PADDING]\n",
    "                    example[sentence + '_index_sequence'][i] = index\n",
    "    return indices_to_words, word_indices\n",
    "    \n",
    "indices_to_words, word_indices = sentences_to_padded_index_sequences([training_set, dev_set, test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print training_set[6]\n",
    "print len(word_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load GloVe. You'll need the same file that you used for the in-class exercise on word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "glove_home = '../'\n",
    "words_to_load = 50000\n",
    "\n",
    "with open(glove_home + 'glove.6B.50d.txt') as f:\n",
    "    loaded_embeddings = np.zeros((len(word_indices), 50), dtype='float32')\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        \n",
    "        s = line.split()\n",
    "        if s[0] in word_indices:\n",
    "            loaded_embeddings[word_indices[s[0]], :] = np.asarray(s[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up an evaluation function as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, eval_set):\n",
    "    correct = 0\n",
    "    hypotheses = classifier(eval_set)\n",
    "    for i, example in enumerate(eval_set):\n",
    "        hypothesis = hypotheses[i]\n",
    "        if hypothesis == example['label']:\n",
    "            correct += 1        \n",
    "    return correct / float(len(eval_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementation (70%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand the below starter code to build a working attention-based NLI model. The model should feature the following:\n",
    "\n",
    "- 50D word embeddings initialized with GloVe and trained. (Using self.E should provide this.)\n",
    "- Three GRUs (with two different sets of parameters): a premise encoder, a hypothesis encoder (sharing the same parameters), and matching GRU, taking the place of [Wang and Jiang](https://arxiv.org/pdf/1512.08849v2.pdf)'s mLSTM, with its own parameters. There shouldn't be a connection between the premise and hypothesis GRUs: the starting hidden state for each of the three GRUs should be zeros.\n",
    "- Word-by-word attention using [Luong et al.](http://www.aclweb.org/anthology/D15-1166)'s 'general' (bilinear) scoring function.\n",
    "- A three-way softmax classifier whose input is the final hidden state of the matching RNN.\n",
    "\n",
    "You are welcome to use code from solutions to past exercises. As ever, do not use any other pre-written code or TF functions that are specific to RNNs or attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in class, the below line may give you a warning. It should be safe to ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class RNNEntailmentClassifier:\n",
    "    def __init__(self, vocab_size, sequence_length):\n",
    "        # Define the hyperparameters\n",
    "        self.training_epochs = 30  # How long to train for - chosen to fit within class time\n",
    "        self.display_epoch_freq = 1  # How often to print cost (in epochs)\n",
    "        self.display_step_freq = 250  # How often to test (in steps)\n",
    "        self.dim = 24  # The dimension of the hidden state of each RNN\n",
    "        self.embedding_dim = 50  # The dimension of the learned word embeddings\n",
    "        self.batch_size = 64  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n",
    "        self.vocab_size = vocab_size  # Defined by the file reader above\n",
    "        self.sequence_length = sequence_length  # Defined by the file reader above\n",
    "        \n",
    "        # Define the parameters\n",
    "        self.E = tf.Variable(loaded_embeddings, trainable=False)\n",
    "        \n",
    "        # TODO: Define the rest of the parameters\n",
    "        \n",
    "        # Define the placeholders\n",
    "        self.premise_x = tf.placeholder(tf.int32, [None, self.sequence_length])\n",
    "        self.hypothesis_x = tf.placeholder(tf.int32, [None, self.sequence_length])\n",
    "        self.y = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        # TODO: Define the model\n",
    "        \n",
    "        # TODO: Define the logits here\n",
    "        self.logits = None\n",
    "        \n",
    "        # Define the cost function (here, the softmax exp and sum are built in)\n",
    "        self.total_cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(self.logits, self.y))\n",
    "        \n",
    "        # This  performs the main SGD update equation with gradient clipping\n",
    "        optimizer_obj = tf.train.AdamOptimizer()\n",
    "        gvs = optimizer_obj.compute_gradients(self.total_cost)\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, 5.0), var) for grad, var in gvs if grad is not None]\n",
    "        self.optimizer = optimizer_obj.apply_gradients(capped_gvs)\n",
    "        \n",
    "        # Create an operation to fill zero values in for W and b\n",
    "        self.init = tf.initialize_all_variables()\n",
    "\n",
    "        # Create a placeholder for the session that will be shared between training and evaluation\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        # Initialize variables\n",
    "        self.sess.run(self.init)\n",
    "        self.step = 1\n",
    "        self.epoch = 0\n",
    "        \n",
    "    def train(self, training_data, dev_data):\n",
    "        def get_minibatch(dataset, start_index, end_index):\n",
    "            indices = range(start_index, end_index)\n",
    "            premise_vectors = np.vstack([dataset[i]['sentence1_binary_parse_index_sequence'] for i in indices])\n",
    "            hypothesis_vectors = np.vstack([dataset[i]['sentence2_binary_parse_index_sequence'] for i in indices])\n",
    "            labels = [dataset[i]['label'] for i in indices]\n",
    "            return premise_vectors, hypothesis_vectors, labels\n",
    "\n",
    "        print 'Training.'\n",
    "\n",
    "        # Training cycle\n",
    "        for _ in range(self.training_epochs):\n",
    "            random.shuffle(training_data)\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(training_data) / self.batch_size)\n",
    "            \n",
    "            # Loop over all batches in epoch\n",
    "            for i in range(total_batch):\n",
    "                # Assemble a minibatch of the next B examples\n",
    "                minibatch_premise_vectors, minibatch_hypothesis_vectors, minibatch_labels = get_minibatch(\n",
    "                    training_data, self.batch_size * i, self.batch_size * (i + 1))\n",
    "\n",
    "                # Run the optimizer to take a gradient step, and also fetch the value of the \n",
    "                # cost function for logging\n",
    "                _, c = self.sess.run(\n",
    "                     [self.optimizer, self.total_cost], \n",
    "                     feed_dict={self.premise_x: minibatch_premise_vectors,\n",
    "                                self.hypothesis_x: minibatch_hypothesis_vectors,\n",
    "                                self.y: minibatch_labels})\n",
    "\n",
    "                if self.step % self.display_step_freq == 0:\n",
    "                    print \"Step:\", self.step, \"Dev acc:\", evaluate_classifier(self.classify, dev_data[0:1000]), \\\n",
    "                        \"Train acc:\", evaluate_classifier(self.classify, training_data[0:1000]) \n",
    "                                  \n",
    "                self.step += 1\n",
    "                avg_cost += c / (total_batch * self.batch_size)\n",
    "                                \n",
    "            # Display some statistics about the step\n",
    "            # Evaluating only one batch worth of data -- simplifies implementation slightly\n",
    "            if self.epoch % self.display_epoch_freq == 0:\n",
    "                print \"Epoch:\", (self.epoch + 1), \"Cost:\", avg_cost \n",
    "            self.epoch += 1\n",
    "    \n",
    "    def classify(self, examples):\n",
    "        # This classifies a list of examples\n",
    "        premise_vectors = np.vstack([example['sentence1_binary_parse_index_sequence'] for example in examples])\n",
    "        hypothesis_vectors = np.vstack([example['sentence2_binary_parse_index_sequence'] for example in examples])\n",
    "        logits = self.sess.run(self.logits,\n",
    "                               feed_dict={self.premise_x: premise_vectors,\n",
    "                                          self.hypothesis_x: hypothesis_vectors})\n",
    "\n",
    "        return np.argmax(logits, axis=1)\n",
    "\n",
    "    def get_attn(self, examples):\n",
    "        premise_vectors = np.vstack([example['sentence1_binary_parse_index_sequence'] for example in examples])\n",
    "        hypothesis_vectors = np.vstack([example['sentence2_binary_parse_index_sequence'] for example in examples])\n",
    "        attn_weights = self.sess.run(self.complete_attn_weights, \n",
    "                                     feed_dict={self.premise_x: premise_vectors,\n",
    "                                                self.hypothesis_x: hypothesis_vectors})\n",
    "        return np.reshape(attn_weights, [len(examples), 10, 10])\n",
    "        \n",
    "    def plot_attn(self, examples):\n",
    "        attn_weights = self.get_attn(examples)\n",
    "\n",
    "        for i in range(len(examples)):    \n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111)\n",
    "            ax.matshow(attn_weights[i,:,:], vmin=0., vmax=1., cmap=plt.cm.inferno)\n",
    "\n",
    "            premise_tokens = [indices_to_words[index] for index in examples[i]['sentence1_binary_parse_index_sequence']]\n",
    "            hypothesis_tokens = [indices_to_words[index] for index in examples[i]['sentence2_binary_parse_index_sequence']]\n",
    "\n",
    "            ax.set_yticklabels(premise_tokens)\n",
    "            ax.set_xticklabels(hypothesis_tokens, rotation=45)\n",
    "\n",
    "            plt.xticks(np.arange(0, 10, 1.0))\n",
    "            plt.yticks(np.arange(0, 10, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a classifier and initializes it. Note that starting and stopping training will not re-initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier = RNNEntailmentClassifier(len(word_indices), SEQ_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this line tests that you've successfully avoided a common error in implementing attention: mixing up dimensions in a way that allows information about one example in a batch to impact the results for other examples. It should do nothing if the model behaves correctly, and will throw an error otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert (classifier.get_attn(training_set[0:2])[0, :, :] == \\\n",
    "        classifier.get_attn(training_set[0:3])[0, :, :]).all(), \\\n",
    "       'Warning: There is cross-example information flow.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Run the model below. Your goal is dev set performance above 74% at at least a few steps within the first 30 epochs. This may require running your model overnight, though it won't necessarily require running for all 30 epochs. Since a single epoch can take a long time with this model, you'll see accuracy results every 250 steps rather than every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier.train(training_set, dev_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will print some (NYU-colored) visualizations for the first ten training examples. It should be useful in the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier.plot_attn(training_set[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Questions (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Consider a version of our model that discards the attention scoring function, and instead always allocates all of its attention on a single word, where that single word is selected by a simple strategy that does not depend on the contents of the hypothesis sentence. With the right such strategy, this version of our model would still be able to learn to solve NLI well (i.e., with better than 80% dev accuracy), though we might need to use larger hidden states in our GRU. What simple strategy would we need to use and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Looking at a visualization like the ones generated above, it is usually possible to identify which axis contains the hypothesis words and which axis contains the premise words. This is possible even if you look only at the plot itself and disregard the words. What property of the plot allows you do this? Under what circumstances would such a strategy fail to give you an answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Using the visualization tool above, and any additional evaluation tools you find useful, show one example each of a case where...\n",
    "  - ...the model uses attention in a reasonable way and gets a correct answer.\n",
    "  - ...the model uses attention in an unreasonable/unhelpful way, but gets a correct answer anyway.\n",
    "  - ...the model uses attention in a reasonable way, but gets an incorrect answer.\n",
    "  - ...the model uses attention in an unreasonable/unhelpful way and gets an incorrect answer.\n",
    "  \n",
    "You should make sure that these four examples are displayed one after another, and you should include some commentary on what role you think attention might be playing in each decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
