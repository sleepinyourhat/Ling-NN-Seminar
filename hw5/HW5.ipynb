{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5: A GRU-pair model for SNLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we'll build train a GRU RNN-based model for SNLI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to download and unzip SNLI, which you can find [here](http://nlp.stanford.edu/projects/snli/). Set `snli_home` below to point to it. The following block of code loads it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snli_home = '../snli_1.0'\n",
    "\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "\n",
    "LABEL_MAP = {\n",
    "    \"entailment\": 0,\n",
    "    \"neutral\": 1,\n",
    "    \"contradiction\": 2\n",
    "}\n",
    "\n",
    "def load_snli_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            loaded_example = json.loads(line)\n",
    "            if loaded_example[\"gold_label\"] not in LABEL_MAP:\n",
    "                continue\n",
    "            loaded_example[\"label\"] = LABEL_MAP[loaded_example[\"gold_label\"]]\n",
    "            data.append(loaded_example)\n",
    "        random.seed(1)\n",
    "        random.shuffle(data)\n",
    "    return data\n",
    "     \n",
    "training_set = load_snli_data(snli_home + '/snli_1.0_train.jsonl')\n",
    "dev_set = load_snli_data(snli_home + '/snli_1.0_dev.jsonl')\n",
    "test_set = load_snli_data(snli_home + '/snli_1.0_test.jsonl')\n",
    "\n",
    "# Note: Unlike with k-nearest neighbors, evaluation here should be fast, and we don't need to\n",
    "# trim down the dev and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll convert the data to index vectors in the same way that we've done for in-class exercises with RNN-based sentiment models. A few notes:\n",
    "\n",
    "- We use a sequence length of only 10, which is short enough that we're truncating a large fraction of sentences.\n",
    "- Tokenization is easy here because we're relying on the output of a parser (which does tokenization as part of parsing), just as with the SST corpus that we've been using until now. Note that we use the 'sentence1_binary_parse' field of each example rather than the human-readable 'sentence1'.\n",
    "- We're using a moderately large vocabulary (for a class exercise) of about 12k words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 10\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def sentences_to_padded_index_sequences(datasets):\n",
    "    '''Annotates datasets with feature vectors.'''\n",
    "    \n",
    "    PADDING = \"<PAD>\"\n",
    "    UNKNOWN = \"<UNK>\"\n",
    "    \n",
    "    # Extract vocabulary\n",
    "    def tokenize(string):\n",
    "        string = re.sub(r'\\(|\\)', '', string)\n",
    "        return string.lower().split()\n",
    "    \n",
    "    word_counter = collections.Counter()\n",
    "    for example in datasets[0]:\n",
    "        word_counter.update(tokenize(example['sentence1_binary_parse']))\n",
    "        word_counter.update(tokenize(example['sentence2_binary_parse']))\n",
    "        \n",
    "    vocabulary = set([word for word in word_counter if word_counter[word] > 10])\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary = [PADDING, UNKNOWN] + vocabulary\n",
    "        \n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    indices_to_words = {v: k for k, v in word_indices.items()}\n",
    "        \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            for sentence in ['sentence1_binary_parse', 'sentence2_binary_parse']:\n",
    "                example[sentence + '_index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n",
    "\n",
    "                token_sequence = tokenize(example[sentence])\n",
    "                padding = SEQ_LEN - len(token_sequence)\n",
    "\n",
    "                for i in range(SEQ_LEN):\n",
    "                    if i >= padding:\n",
    "                        if token_sequence[i - padding] in word_indices:\n",
    "                            index = word_indices[token_sequence[i - padding]]\n",
    "                        else:\n",
    "                            index = word_indices[UNKNOWN]\n",
    "                    else:\n",
    "                        index = word_indices[PADDING]\n",
    "                    example[sentence + '_index_sequence'][i] = index\n",
    "    return indices_to_words, word_indices\n",
    "    \n",
    "indices_to_words, word_indices = sentences_to_padded_index_sequences([training_set, dev_set, test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print training_set[6]\n",
    "print len(word_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load GloVe. You'll need the same file that you used for the in-class exercise on word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove_home = '../'\n",
    "words_to_load = 25000\n",
    "\n",
    "with open(glove_home + 'glove.6B.50d.txt') as f:\n",
    "    loaded_embeddings = np.zeros((len(word_indices), 50), dtype='float32')\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        \n",
    "        s = line.split()\n",
    "        if s[0] in word_indices:\n",
    "            loaded_embeddings[word_indices[s[0]], :] = np.asarray(s[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up an evaluation function as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, eval_set):\n",
    "    correct = 0\n",
    "    hypotheses = classifier(eval_set)\n",
    "    for i, example in enumerate(eval_set):\n",
    "        hypothesis = hypotheses[i]\n",
    "        if hypothesis == example['label']:\n",
    "            correct += 1        \n",
    "    return correct / float(len(eval_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementation (70%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand the below starter code to build a GRU RNN-pair NLI model. The model should feature the following:\n",
    "\n",
    "- 50D word embeddings initialized with GloVe and trained. (Using self.E should provide this.)\n",
    "- Two GRUs (sharing one set of parameters) that read each sentence independantly and produce one $\\vec{h}_t$ vector for each. We'll call these two vectors $\\vec{h}_{p}$ for the premise (first sentence) and $\\vec{h}_{h}$ for the hypothesis (second sentence).\n",
    "- A combination layer in the style of [Mou et al. 15](https://arxiv.org/abs/1512.08422)'s heuristic matching layer: A ReLU layer with the following four vectors as inputs:\n",
    "  - $\\vec{h}_p$, $\\vec{h}_h$, $\\vec{h}_p - \\vec{h}_t$, $\\vec{h}_p * \\vec{h}_t$\n",
    "- A three-way softmax classifier whose inputs are the outputs of the combination layer.\n",
    "\n",
    "As in the previous assignment, you may use code from in-class exercises, but you should not use any specialized TF functions for LSTMs or RNNs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNNEntailmentClassifier:\n",
    "    def __init__(self, vocab_size, sequence_length):\n",
    "        # Define the hyperparameters\n",
    "        self.learning_rate = 1.0  # Should be about right\n",
    "        self.training_epochs = 25  # How long to train for - chosen to fit within class time\n",
    "        self.display_epoch_freq = 1  # How often to test and print out statistics\n",
    "        self.dim = 16  # The dimension of the hidden state of the RNN\n",
    "        self.combination_dim = 32  # The dimension of the hidden state of the combination layer\n",
    "        self.embedding_dim = 50  # The dimension of the learned word embeddings\n",
    "        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n",
    "        self.vocab_size = vocab_size  # Defined by the file reader above\n",
    "        self.sequence_length = sequence_length  # Defined by the file reader above\n",
    "        \n",
    "        # Define the parameters\n",
    "        self.E = tf.Variable(loaded_embeddings)\n",
    "                \n",
    "        self.W_cl = tf.Variable(tf.random_normal([self.combination_dim, 3], stddev=0.1))\n",
    "        self.b_cl = tf.Variable(tf.random_normal([3], stddev=0.1))\n",
    "        \n",
    "        # TODO: Define the rest of the parameters\n",
    "        \n",
    "        # Define the placeholders\n",
    "        self.premise_x = tf.placeholder(tf.int32, [None, self.sequence_length])\n",
    "        self.hypothesis_x = tf.placeholder(tf.int32, [None, self.sequence_length])\n",
    "        self.y = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        # Split up the inputs into individual tensors\n",
    "        self.x_premise_slices = tf.split(1, self.sequence_length, self.premise_x)\n",
    "        self.x_hypothesis_slices = tf.split(1, self.sequence_length, self.hypothesis_x)\n",
    "        \n",
    "        # TODO:  Define one step of the RNN\n",
    "        \n",
    "        # TODO: Unroll the first RNN\n",
    "        \n",
    "        # TODO: Unroll the second RNN\n",
    "        \n",
    "        # TODO: Build the combination layer\n",
    "        \n",
    "        # TODO: Compute the logits\n",
    "        \n",
    "        # Define the cost function (here, the softmax exp and sum are built in)\n",
    "        self.total_cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(self.logits, self.y))\n",
    "        \n",
    "        # This  performs the main SGD update equation with gradient clipping\n",
    "        optimizer_obj = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n",
    "        gvs = optimizer_obj.compute_gradients(self.total_cost)\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, 5.0), var) for grad, var in gvs if grad is not None]\n",
    "        self.optimizer = optimizer_obj.apply_gradients(capped_gvs)\n",
    "        \n",
    "        # Create an operation to fill zero values in for W and b\n",
    "        self.init = tf.initialize_all_variables()\n",
    "        \n",
    "        # Create a placeholder for the session that will be shared between training and evaluation\n",
    "        self.sess = None\n",
    "        \n",
    "    def train(self, training_data, dev_data):\n",
    "        def get_minibatch(dataset, start_index, end_index):\n",
    "            indices = range(start_index, end_index)\n",
    "            premise_vectors = np.vstack([dataset[i]['sentence1_binary_parse_index_sequence'] for i in indices])\n",
    "            hypothesis_vectors = np.vstack([dataset[i]['sentence2_binary_parse_index_sequence'] for i in indices])\n",
    "            labels = [dataset[i]['label'] for i in indices]\n",
    "            return premise_vectors, hypothesis_vectors, labels\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.sess.run(self.init)\n",
    "        print 'Training.'\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(self.training_epochs):\n",
    "            random.shuffle(training_data)\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(training_data) / self.batch_size)\n",
    "            \n",
    "            # Loop over all batches in epoch\n",
    "            for i in range(total_batch):\n",
    "                # Assemble a minibatch of the next B examples\n",
    "                minibatch_premise_vectors, minibatch_hypothesis_vectors, minibatch_labels = get_minibatch(\n",
    "                    training_data, self.batch_size * i, self.batch_size * (i + 1))\n",
    "\n",
    "                # Run the optimizer to take a gradient step, and also fetch the value of the \n",
    "                # cost function for logging\n",
    "                _, c = self.sess.run([self.optimizer, self.total_cost], \n",
    "                                     feed_dict={self.premise_x: minibatch_premise_vectors,\n",
    "                                                self.hypothesis_x: minibatch_hypothesis_vectors,\n",
    "                                                self.y: minibatch_labels})\n",
    "                                                                    \n",
    "                # Compute average loss\n",
    "                avg_cost += c / (total_batch * self.batch_size)\n",
    "                                \n",
    "            # Display some statistics about the step\n",
    "            # Evaluating only one batch worth of data -- simplifies implementation slightly\n",
    "            if (epoch+1) % self.display_epoch_freq == 0:\n",
    "                print \"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \\\n",
    "                    \"Dev acc:\", evaluate_classifier(self.classify, dev_data[0:1000]), \\\n",
    "                    \"Train acc:\", evaluate_classifier(self.classify, training_data[0:1000])  \n",
    "    \n",
    "    def classify(self, examples):\n",
    "        # This classifies a list of examples\n",
    "        premise_vectors = np.vstack([example['sentence1_binary_parse_index_sequence'] for example in examples])\n",
    "        hypothesis_vectors = np.vstack([example['sentence2_binary_parse_index_sequence'] for example in examples])\n",
    "        logits = self.sess.run(self.logits, feed_dict={self.premise_x: premise_vectors,\n",
    "                                                       self.hypothesis_x: hypothesis_vectors})\n",
    "        return np.argmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Run the model below. Your goal is dev set performance above 75% within the first 25 epochs, though a successful model will likely reach about 77%.\n",
    "\n",
    "Since epochs over the half-million example SNLI corpus are slow, you may wish to debug your model using only a small subset of SNLI by passing training_set[:10000] into classifier.train as its first argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = RNNEntailmentClassifier(len(word_indices), SEQ_LEN)\n",
    "classifier.train(training_set, dev_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Questions (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Focusing only on the performance of your model on the first ten epochs of training, would adding L2 regularization or dropout help your dev set performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Write a short script to test the model's performance on dev set examples that contain sentences of more than ten words and those that don't contain such sentences. How does length impact performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** The combination layer uses four different types of input feature. If we skipped the combination layer and fed these features into the softmax classifier layer directly, one of these features would become almost entirely uninformative. Which one is it? (Hint: This is true with SNLI, but may not be true with other corpora.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
